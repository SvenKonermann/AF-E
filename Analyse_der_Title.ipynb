{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdc58fe",
   "metadata": {},
   "source": [
    "# __Analyse der Titel__\n",
    "\n",
    "In diesem Schritt werden die meistgenannten Titel identifiziert und in ein Wörterbuch überführt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b65f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "import os\n",
    "\n",
    "# Working-Directory\n",
    "os.chdir('C:/Users/Sven Konermann/Documents/Master/2_Semester/aF_E_Faelle/Unterlagen/00_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb87214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15625, 17)\n"
     ]
    }
   ],
   "source": [
    "# Einlesen der Daten über ein csv File\n",
    "data = pd.read_csv(\"Inserate_clean_Titelanalyse_AF_E.csv\", sep=\";\")\n",
    "print(data.shape)\n",
    "\n",
    "# Auf Null-Werte prüfen und diese löschen\n",
    "data = data[~data['Title_Anforderungen'].isnull()]\n",
    "\n",
    "# Erneutes Tokenisieren\n",
    "data['title_tok'] = data.apply(lambda row: nltk.word_tokenize(row['Title_Anforderungen']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e55168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jahr auswählen, für das ein Wörterbuch erstellt werden soll\n",
    "data = data.loc[data.year == 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbfa4d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(business, analyst)                273\n",
       "(application, manager)             136\n",
       "(80, 100)                           92\n",
       "(application, engineer)             70\n",
       "(inhouse, consultant)               62\n",
       "(consultant, mw)                    62\n",
       "(mwd, sap)                          62\n",
       "(sachbearbeiterin, buchhaltung)     60\n",
       "(mw, sap)                           55\n",
       "(data, analyst)                     53\n",
       "(consultant, sap)                   44\n",
       "(mw, 100)                           40\n",
       "(100, mw)                           40\n",
       "(mw, 80100)                         39\n",
       "(sap, consultant)                   38\n",
       "(analyst, mw)                       35\n",
       "(senior, consultant)                34\n",
       "(data, engineer)                    34\n",
       "(system, engineer)                  34\n",
       "(it, projektleiter)                 34\n",
       "(business, analystin)               33\n",
       "(project, manager)                  32\n",
       "(manager, mw)                       31\n",
       "(it, business)                      30\n",
       "(hr, manager)                       28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finden von oft vorkommenden Phrasen mit einer Länge zwischen 2 und 4 Wörtern\n",
    "from nltk import ngrams\n",
    "vals = [y for x in data['Title_Anforderungen'] for y in x.split()]\n",
    "\n",
    "n = [2,3,4]\n",
    "a = pd.Series([y for x in n for y in ngrams(vals, x)]).value_counts()\n",
    "a.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abspeichern in ein CSV als Basis für das Wörterbuch\n",
    "a.to_csv(\"ngram_count_all_Title.csv\", encoding=\"utf-8\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einmal vorkommende Wörter in Set abspeichern\n",
    "results = set()\n",
    "data['title_tok'].apply(results.update)\n",
    "n = results\n",
    "len(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleiche das Vorkommen von unique words im Text_new (Unique words list kommt vom Tokanizing oben).\n",
    "# Danach ausschreiben der Begriffe, welche Anforderungen entsprechen, in ein Wörterbuch\n",
    "wordsets = [ frozenset(document.split(' ')) for document in data['Title_Anforderungen'] ]\n",
    "results = []\n",
    "for word in n:\n",
    "    count = sum( 1 for s in wordsets if word in s )\n",
    "    results.append((count, word))\n",
    "for count, word in sorted(results, reverse=True):\n",
    "    if count >= 25: # Mindestanzahl Nennungen der Begriffe\n",
    "        print(word, \";\", count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
