{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Datenbereinigung Teil 2: Bereinigung der Anforderungsdaten__\n",
    "\n",
    "In diesem Schritt werden die Daten bereinigt, welche für das Wörterbuch mit den häufigst genannten Anforderungen benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der benötigten Libraries\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from bs4 import BeautifulSoup\n",
    "from HanTa import HanoverTagger as ht\n",
    "import string\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import os\n",
    "\n",
    "# Working-Directory\n",
    "os.chdir('C:/Users/Sven Konermann/Documents/Master/2_Semester/aF_E_Faelle/Unterlagen/00_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41924, 7)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41924 entries, 0 to 41923\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       41924 non-null  int64 \n",
      " 1   ad_id            41924 non-null  object\n",
      " 2   active_start_on  41924 non-null  object\n",
      " 3   active_end_on    41924 non-null  object\n",
      " 4   title            41924 non-null  object\n",
      " 5   text             41923 non-null  object\n",
      " 6   year             41924 non-null  int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Einlesen der Daten über ein csv File\n",
    "data = pd.read_csv(\"Inserate_GEsamtdatenanalyse.csv\", sep=\";\")\n",
    "print(data.shape)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      6\u001b[0m     text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     tags \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#print(tags)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:312\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     markup \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    313\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    315\u001b[0m ):\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# Print out warnings for a couple beginner problems\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;66;03m# involving passing non-markup to Beautiful Soup.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# just in case that's what the user really wants.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msupports_unicode_filenames):\n\u001b[0;32m    322\u001b[0m         possible_filename \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "# Listenelemente in einer neuen Spalte abspeichern\n",
    "start = time()\n",
    "\n",
    "column = []\n",
    "for i in data['text']:\n",
    "    text=''\n",
    "    soup = BeautifulSoup(i, 'lxml')\n",
    "    tags = soup.find_all('li')\n",
    "    #print(tags)\n",
    "    for t in tags:\n",
    "        text = text + ' ' + t.text \n",
    "    column.append(text)\n",
    "\n",
    "data['Text_Anforderungen'] = column\n",
    "\n",
    "print(f'Benötigte Zeit: {time() - start} Sekunden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benötigte Zeit: 3253.0038492679596 Sekunden\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisierung der Anforderungen\n",
    "start = time()\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "\n",
    "for mail in data['Text_Anforderungen']:\n",
    "    lemma = [lemma for (word,lemma,pos) in tagger.tag_sent(mail.split())]\n",
    "    index = data[data['Text_Anforderungen']==mail].index\n",
    "    data.loc[index, 'Text_Anforderungen'] = (' '.join(lemma))\n",
    "    \n",
    "print(f'Benötigte Zeit: {time() - start} Sekunden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text in Kleinbuchstaben umwandeln\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satzzeichen löschen\n",
    "def remove_punctuation(txt):\n",
    "    txt_nopunct = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    return txt_nopunct\n",
    "\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        leitung des projekt sap nebenbuchhaltung fç¬r ...\n",
       "1        abschluss in wirtschaftsinformatik oder gleich...\n",
       "2        leiten und überwachen von it und organisations...\n",
       "3        mehrjçïhrig und ausgewiesen erfahrung als it g...\n",
       "4        verantwortlich fç¬r das unternehmensintern inf...\n",
       "                               ...                        \n",
       "25095    debitoren und kreditorenbuchhaltung verrechnun...\n",
       "25096    erfassen und bewirtschaften von klientendaten ...\n",
       "25097    führung der finanzbuchhaltung von zwei gesells...\n",
       "25098    unterstützung debitoren  kreditoren unterstütz...\n",
       "25099    koordination und begleitung der buchhaltungspr...\n",
       "Name: Text_Anforderungen, Length: 25100, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tabs \\t aus den Texten entfernen\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"\\t\", \"\", regex=True)\n",
    "data['Text_Anforderungen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ähnliche Anforderungen manuell mit einem Hauptbegriff ersetzen\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"teamplayer\", \"teamfähig\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"verhandlungssicher\", \"verhandlungsgeschick\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"kundenorientierte\", \"kundenorientierung\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"projekt management\", \"projektleitungserfahrung\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"projektmanagement\", \"projektleitungserfahrung\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"zusammenarbeit\", \"teamfähig\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"kommunikation\", \"kommunikationsfähigkeit\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"kommunikationsfähigkeiten\", \"kommunikationsfähigkeit\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"dwh\", \"data warehouse\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"dienstleistungsorientierung\", \"dienstleistungsorientiert\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"eigenverantwortung\", \"eigenverantwortlich\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benötigte Zeit: 12.778288125991821 Sekunden\n"
     ]
    }
   ],
   "source": [
    "# Word-Tokenization \n",
    "#nltk.download('punkt')\n",
    "\n",
    "start = time()\n",
    "data['text_tok'] = data.apply(lambda row: nltk.word_tokenize(row['Text_Anforderungen']), axis=1)\n",
    "print(f'Benötigte Zeit: {time() - start} Sekunden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sven\n",
      "[nltk_data]     Konermann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopwörter löschen\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('german'))\n",
    "data['text_tok'] = data['text_tok'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing rückgängig machen, da die Tokenizierung nicht in eine csv-Datei überführt werden kann\n",
    "column = []\n",
    "for i in data['text_tok']:\n",
    "    test = TreebankWordDetokenizer().detokenize(i)\n",
    "    column.append(test)\n",
    "\n",
    "data['Text_Anforderungen'] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Löschen der tokenisierten Spalte\n",
    "data = data.drop(['text_tok'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erneutes Löschen der Duplikate, da sich die Texte verändert haben\n",
    "data = data.drop_duplicates(subset='Text_Anforderungen', keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"Inserate_clean_Anforderungsanalyse.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nächster Schritt: Wörterbuch erstellen (Analyse_der_Anforderungen.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
