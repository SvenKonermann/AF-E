{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Datenbereinigung Teil 2: Bereinigung der Anforderungsdaten__\n",
    "\n",
    "In diesem Schritt werden die Daten bereinigt, welche für das Wörterbuch mit den häufigst genannten Anforderungen benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der benötigten Libraries\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from bs4 import BeautifulSoup\n",
    "from HanTa import HanoverTagger as ht\n",
    "import string\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import os\n",
    "\n",
    "# Working-Directory\n",
    "os.chdir('C:/Users/Sven Konermann/Documents/Master/2_Semester/aF_E_Faelle/Unterlagen/JobCloud_Daten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75980, 14)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75980 entries, 0 to 75979\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Unnamed: 0             75980 non-null  int64 \n",
      " 1   ad_id                  75980 non-null  object\n",
      " 2   active_start_on        75980 non-null  object\n",
      " 3   active_end_on          75980 non-null  object\n",
      " 4   title                  75980 non-null  object\n",
      " 5   text                   75980 non-null  object\n",
      " 6   branchen_name          75539 non-null  object\n",
      " 7   customer_branche_name  75978 non-null  object\n",
      " 8   lvl1_region_name       75950 non-null  object\n",
      " 9   region_name            75950 non-null  object\n",
      " 10  lvl1_jobcategory_name  75965 non-null  object\n",
      " 11  jobcategory_name       75965 non-null  object\n",
      " 12  duration               75980 non-null  int64 \n",
      " 13  year                   75980 non-null  int64 \n",
      "dtypes: int64(3), object(11)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Einlesen der Daten über ein csv File\n",
    "data = pd.read_csv(\"Inserate_Anforderungsanalyse_AF_E.csv\", sep=\";\")\n",
    "print(data.shape)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benötigte Zeit: 71.63456702232361 Sekunden\n"
     ]
    }
   ],
   "source": [
    "# Listenelemente in einer neuen Spalte abspeichern\n",
    "start = time()\n",
    "\n",
    "column = []\n",
    "for i in data['text']:\n",
    "    text=''\n",
    "    soup = BeautifulSoup(i, 'lxml')\n",
    "    tags = soup.find_all('li')\n",
    "    #print(tags)\n",
    "    for t in tags:\n",
    "        text = text + ' ' + t.text \n",
    "    column.append(text)\n",
    "\n",
    "data['Text_Anforderungen'] = column\n",
    "\n",
    "print(f'Benötigte Zeit: {time() - start} Sekunden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benötigte Zeit: 8474.594832658768 Sekunden\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisierung der Anforderungen\n",
    "start = time()\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "\n",
    "for mail in data['Text_Anforderungen']:\n",
    "    lemma = [lemma for (word,lemma,pos) in tagger.tag_sent(mail.split())]\n",
    "    index = data[data['Text_Anforderungen']==mail].index\n",
    "    data.loc[index, 'Text_Anforderungen'] = (' '.join(lemma))\n",
    "    \n",
    "print(f'Benötigte Zeit: {time() - start} Sekunden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text in Kleinbuchstaben umwandeln\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satzzeichen löschen\n",
    "def remove_punctuation(txt):\n",
    "    txt_nopunct = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    return txt_nopunct\n",
    "\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        teil in verschieden softwareapplikation erfass...\n",
       "1        warenannahme kennzeichnung und einlagerung der...\n",
       "2        termin und reiseplanung inkl flugbuchung und s...\n",
       "3        vertretung des abteilungsleiter bei sein abwes...\n",
       "4        erarbeiten von business analyse für unser soft...\n",
       "                               ...                        \n",
       "75975    du sein verantwortlich für die schadenregulier...\n",
       "75976    erhebung der anamnese beurteilung der medizini...\n",
       "75977    diverser administrativ tätigkeit z b pflege de...\n",
       "75978    selbständig führung der finanz betriebs und an...\n",
       "75979    spenglerarbeit auf steil und flachdächern werk...\n",
       "Name: Text_Anforderungen, Length: 75980, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tabs \\t aus den Texten entfernen\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"\\t\", \"\", regex=True)\n",
    "data['Text_Anforderungen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ähnliche Anforderungen manuell mit einem Hauptbegriff ersetzen\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"teamplayer\", \"teamfähig\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"verhandlungssicher\", \"verhandlungsgeschick\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"kundenorientierte\", \"kundenorientierung\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"projekt management\", \"projektleitungserfahrung\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"projektmanagement\", \"projektleitungserfahrung\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"zusammenarbeit\", \"teamfähig\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"kommunikation\", \"kommunikationsfähigkeit\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"kommunikationsfähigkeiten\", \"kommunikationsfähigkeit\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"dwh\", \"data warehouse\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"dienstleistungsorientierung\", \"dienstleistungsorientiert\", regex=True)\n",
    "data['Text_Anforderungen'] = data['Text_Anforderungen'].str.replace(\"eigenverantwortung\", \"eigenverantwortlich\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benötigte Zeit: 34.05356812477112 Sekunden\n"
     ]
    }
   ],
   "source": [
    "# Word-Tokenization \n",
    "#nltk.download('punkt')\n",
    "\n",
    "start = time()\n",
    "data['text_tok'] = data.apply(lambda row: nltk.word_tokenize(row['Text_Anforderungen']), axis=1)\n",
    "print(f'Benötigte Zeit: {time() - start} Sekunden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sven\n",
      "[nltk_data]     Konermann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Stopwörter löschen\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('german'))\n",
    "data['text_tok'] = data['text_tok'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing rückgängig machen, da die Tokenizierung nicht in eine csv-Datei überführt werden kann\n",
    "column = []\n",
    "for i in data['text_tok']:\n",
    "    test = TreebankWordDetokenizer().detokenize(i)\n",
    "    column.append(test)\n",
    "\n",
    "data['Text_Anforderungen'] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Löschen der tokenisierten Spalte\n",
    "data = data.drop(['text_tok'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erneutes Löschen der Duplikate, da sich die Texte verändert haben\n",
    "data = data.drop_duplicates(subset='Text_Anforderungen', keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"Inserate_clean_Anforderungsanalyse_AF_E.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nächster Schritt: Wörterbuch erstellen (Analyse_der_Anforderungen.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
